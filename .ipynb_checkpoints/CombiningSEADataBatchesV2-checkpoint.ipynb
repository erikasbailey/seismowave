{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbe82968-ba8a-4105-bfd4-dbfe453573e7",
   "metadata": {},
   "source": [
    "This notebook gathers all the sea data processed through BasicPreprocessing-HourslySeaDataV3.py, which was run eight times, in half year increments between 2018 and 2021, to create one coherent data set which occupies minimum memory.\n",
    "\n",
    "BasicPreprocessing-HourslySeaDataV3.py created sea_data_hourly_2018H1.pickle - sea_data_hourly_2021H2.pickle and contain sea height measured at various latitudes and longitudes, with six months' worth of data stored in each file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1932a328-122d-48f6-9e66-46b5f11289d7",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "818ef768-2be1-4447-b4de-064e85605ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dd55a6-038a-4d24-9742-71c38555f23c",
   "metadata": {},
   "source": [
    "# Define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75018067-6a23-452b-8b62-79fb8eea884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define paths to data and find the files\n",
    "\n",
    "data_path = \"Processed Data\"\n",
    "file_pattern = \"sea_data_hourly*.pickle\"\n",
    "\n",
    "file_list = sorted(glob(os.path.join(data_path, file_pattern)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97cd2c68-70ec-43bb-99b9-a9e5db525e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = [] #list for data\n",
    "processed_frames = {} #dict for batches of data\n",
    "time_orig_dict = {} #dict for batches of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301a38cc-29da-44f2-af23-52f5a6048192",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72ee8786-6473-4737-84ce-390cf1b8914c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#26 January 2025: I realised I entered the wrong dates for 2019H2, 2020H2 and 2021H2 when running the code to generate the raw data\n",
    "#When getting H2 data, I started from June instead of July for three years.\n",
    "#To save time on re-running the scripts from scratch for these dates, I am cropping them at this stage.\n",
    "#This function would not be required if the right dates are entered in the script.\n",
    "\n",
    "#02 February 2025: I had to fix the region of interest in the scripts generating the data, so this function is no longer technically required.\n",
    "\n",
    "def filtertimerange(df_time, period):\n",
    "    if period == '2019H2':\n",
    "        return df_time[(df_time >= pd.Timestamp('2019-07-01'))]\n",
    "    elif period == '2020H2':\n",
    "        return df_time[(df_time >= pd.Timestamp('2020-07-01'))]\n",
    "    elif period == '2021H2':\n",
    "        return df_time[(df_time >= pd.Timestamp('2021-07-01'))]\n",
    "    else:\n",
    "        return df_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6edb642d-ea60-4954-8e9c-8379b117fac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process each pickle file. Y, lat and lon are ndarrays, time is a pd series\n",
    "\n",
    "def get_time_and_Y(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "        \n",
    "    #Extract the period from the filename to check if corpping needs to take place due to the error I made in the dates when generating the data\n",
    "    period = file_path.split('/')[-1].split('_')[3].replace('.pickle', '')\n",
    "    \n",
    "    for key, value in data.items():\n",
    "        #process series (for time)\n",
    "        if key == 'time' and isinstance(value, pd.Series):\n",
    "            #Remove timezone information to facilitate further processing\n",
    "            value = pd.to_datetime(value.apply(lambda x: x.replace(tzinfo=None)))\n",
    "            time_orig = value.copy() #keep copy of time to create a mask and crop Y in the same way\n",
    "            #Crop extra time periods\n",
    "            value = filtertimerange(value, period)\n",
    "            processed_frames[key] = value\n",
    "\n",
    "    #Two for loops are required to force start with time due to dependency of Y on time in cropping\n",
    "    for key, value in data.items():\n",
    "        #handle arrays\n",
    "        if key == 'Y' and value.ndim == 3 and isinstance(value, np.ndarray):\n",
    "            time_mask = time_orig.isin(processed_frames['time']) #Mask based on time\n",
    "            value = value[time_mask]\n",
    "            \n",
    "            value = value.reshape(value.shape[0], -1) #flatten grid\n",
    "            processed_frames[key] = pd.DataFrame(value)\n",
    "    \n",
    "    return processed_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70e54268-bde2-4cf5-8ce6-6d90230bf558",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process files and combine data\n",
    "\n",
    "def combine_pickle_files(file_list):\n",
    "    combined_data = []\n",
    "    combined_time = []\n",
    "    combined_Y = []\n",
    "    for file in file_list:\n",
    "        #extract all the 'Y's and 'time's from each file\n",
    "        print(f\"Processing: {file}\")\n",
    "        file_data = get_time_and_Y(file)\n",
    "        combined_data.append(file_data)\n",
    "        combined_time.append(file_data['time'])\n",
    "        combined_Y.append(file_data['Y'])\n",
    "    \n",
    "    return combined_data, combined_time, combined_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a07160a-f717-4ed7-8d0d-1754bd7f1f99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: Processed Data\\sea_data_hourly_2018H1.pickle\n",
      "Processing: Processed Data\\sea_data_hourly_2018H2.pickle\n",
      "Processing: Processed Data\\sea_data_hourly_2019H1.pickle\n",
      "Processing: Processed Data\\sea_data_hourly_2019H2.pickle\n",
      "Processing: Processed Data\\sea_data_hourly_2020H1.pickle\n",
      "Processing: Processed Data\\sea_data_hourly_2020H2.pickle\n",
      "Processing: Processed Data\\sea_data_hourly_2021H1.pickle\n",
      "Processing: Processed Data\\sea_data_hourly_2021H2.pickle\n"
     ]
    }
   ],
   "source": [
    "combined_data, combined_time, combined_Y = combine_pickle_files(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dcaac64-67d9-42d9-a2de-28ae10a54ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     time\n",
      "0     2018-01-01 00:00:00\n",
      "1     2018-01-01 01:00:00\n",
      "2     2018-01-01 02:00:00\n",
      "3     2018-01-01 03:00:00\n",
      "4     2018-01-01 04:00:00\n",
      "...                   ...\n",
      "35059 2021-12-31 19:00:00\n",
      "35060 2021-12-31 20:00:00\n",
      "35061 2021-12-31 21:00:00\n",
      "35062 2021-12-31 22:00:00\n",
      "35063 2021-12-31 23:00:00\n",
      "\n",
      "[35064 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "combined_time_series = pd.concat(combined_time, ignore_index=True)\n",
    "df_time = pd.DataFrame({'time': combined_time_series})\n",
    "print(df_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "158b5517-8b8f-4ae9-800a-b7e5ae427289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0      1      2      3      4      5      6      7      8      9     \\\n",
      "0      0.637  0.650  0.665  0.681  0.694  0.703  0.708  0.710  0.711  0.713   \n",
      "1      0.637  0.655  0.671  0.689  0.704  0.715  0.722  0.724  0.726  0.729   \n",
      "2      0.623  0.643  0.663  0.682  0.699  0.712  0.720  0.725  0.728  0.733   \n",
      "3      0.602  0.623  0.645  0.667  0.686  0.702  0.712  0.719  0.724  0.729   \n",
      "4      0.567  0.590  0.613  0.638  0.660  0.679  0.692  0.701  0.708  0.715   \n",
      "...      ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
      "35059  1.151  1.177  1.198  1.222  1.241  1.258  1.272  1.282  1.287  1.289   \n",
      "35060  1.127  1.149  1.169  1.191  1.209  1.224  1.238  1.247  1.251  1.254   \n",
      "35061  1.056  1.079  1.097  1.118  1.134  1.149  1.161  1.169  1.172  1.174   \n",
      "35062  1.038  1.059  1.076  1.096  1.111  1.125  1.136  1.143  1.146  1.148   \n",
      "35063  1.015  1.035  1.051  1.070  1.085  1.097  1.108  1.115  1.117  1.119   \n",
      "\n",
      "       ...   9954   9955   9956   9957   9958   9959   9960   9961   9962  \\\n",
      "0      ...  0.995  0.994  0.992  0.990  0.987  0.984  0.980  0.975  0.970   \n",
      "1      ...  0.967  0.965  0.963  0.962  0.960  0.958  0.957  0.954  0.952   \n",
      "2      ...  0.941  0.939  0.938  0.936  0.934  0.933  0.931  0.929  0.927   \n",
      "3      ...  0.917  0.915  0.913  0.912  0.910  0.908  0.907  0.905  0.904   \n",
      "4      ...  0.893  0.891  0.890  0.888  0.887  0.885  0.884  0.883  0.881   \n",
      "...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
      "35059  ...  0.557  0.556  0.555  0.554  0.552  0.551  0.549  0.548  0.547   \n",
      "35060  ...  0.542  0.541  0.540  0.539  0.538  0.537  0.536  0.535  0.535   \n",
      "35061  ...  0.665  0.664  0.664  0.663  0.663  0.663  0.663  0.663  0.664   \n",
      "35062  ...  0.637  0.636  0.635  0.635  0.634  0.633  0.633  0.633  0.634   \n",
      "35063  ...  0.610  0.609  0.609  0.608  0.606  0.605  0.604  0.604  0.605   \n",
      "\n",
      "        9963  \n",
      "0      0.966  \n",
      "1      0.950  \n",
      "2      0.926  \n",
      "3      0.902  \n",
      "4      0.880  \n",
      "...      ...  \n",
      "35059  0.547  \n",
      "35060  0.535  \n",
      "35061  0.665  \n",
      "35062  0.635  \n",
      "35063  0.606  \n",
      "\n",
      "[35064 rows x 9964 columns]\n"
     ]
    }
   ],
   "source": [
    "df_Y = pd.concat([pd.DataFrame(y) for y in combined_Y], ignore_index=True)\n",
    "print(df_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5dd8980-aff0-444d-b9c3-6a651b956862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read lat and lon from one file. These are the same for all the files, so we only need to extract them once (they are the 'map')\n",
    "\n",
    "file_path = \"Processed Data/sea_data_hourly_2018H1.pickle\"\n",
    "with open(file_path, 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "#extract lat and lon as distinct data frames\n",
    "for key, value in data.items():\n",
    "    if key == 'lat' and value.ndim == 2:\n",
    "        lat = pd.DataFrame(value)\n",
    "    elif key == 'lon' and value.ndim == 2:\n",
    "        lon = pd.DataFrame(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dce1e4e-a3c0-49e0-9ba4-8a977541d5e1",
   "metadata": {},
   "source": [
    "# Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d9b4fb8-9a90-4fe4-9287-0ea0e7b94d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to folder Processed Data\n"
     ]
    }
   ],
   "source": [
    "#Export dataset to parquet format - one file for each dataframe\n",
    "\n",
    "df_time.to_parquet('Processed Data/SEA_data_time.parquet', engine='pyarrow', compression='snappy')\n",
    "df_Y.to_parquet('Processed Data/SEA_data_Y.parquet', engine='pyarrow', compression='snappy')\n",
    "lat.to_parquet('Processed Data/SEA_data_lat.parquet', engine='pyarrow', compression='snappy')\n",
    "lon.to_parquet('Processed Data/SEA_data_lon.parquet', engine='pyarrow', compression='snappy')\n",
    "\n",
    "print(\"Data saved to folder Processed Data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
